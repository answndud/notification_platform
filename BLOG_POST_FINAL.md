# 처음부터 정답은 없었다: Redis 먼저 붙이고 Kafka로 완성한 알림 시스템 개선 일지

처음엔 그냥 잘 동작하면 된다고 생각했습니다.
요청 받고 저장하고 처리하면 끝이라고 봤어요.

그런데 트래픽이 조금만 몰리면 응답시간이 튀고, 실패가 나면 어디서 막혔는지 찾는 데 시간이 오래 걸렸습니다.
그때 깨달았습니다.

"아, 지금 필요한 건 새 기술 자랑이 아니라 **버티는 구조**구나."

이 글은 Redis와 Kafka를 한 번에 도입한 성공담이 아닙니다.
레거시 방식으로 시작해서, 쉬운 개선부터 순서대로 적용해본 기록입니다.

---

## 0) 시작점: 레거시 방식으로 만든 첫 버전

초기 구조는 전형적이었습니다.

- API가 요청을 받음
- DB 저장
- 처리 로직까지 같은 경로에서 진행

작은 규모에서는 빠르게 만들 수 있어서 좋았습니다.
문제는 "기능이 된다"와 "운영이 된다"는 다르다는 점이었죠.

피크 시간대에 보였던 증상은 이랬습니다.

- API p95가 들쭉날쭉
- 읽기 쿼리 폭증
- 장애 시 복구 동선이 길고 사람 의존적

---

## 1) 왜 Kafka가 아니라 Redis를 먼저 붙였나

처음부터 Kafka를 넣지 않은 이유는 단순합니다.
제가 그때 해결해야 했던 가장 큰 pain point가 "처리 파이프라인" 이전에 "읽기 비용"이었기 때문입니다.

즉, 난이도 높은 구조 변경보다 먼저,
**빠르게 효과를 볼 수 있는 단계**를 선택했습니다.

Redis 1차 적용 내용:

- 자주 조회되는 상태/요약 데이터 캐시
- 키 규칙 표준화(`notification:*`)
- TTL 강제

### Redis 적용 전/후 (가정 시나리오 데이터)

| 지표 | 적용 전(레거시) | Redis 적용 후 | 변화 |
|---|---:|---:|---:|
| API 평균 응답시간 | 310ms | 185ms | -40.3% |
| API p95 응답시간 | 780ms | 460ms | -41.0% |
| DB 읽기 QPS(피크) | 1,200 | 690 | -42.5% |
| 타임아웃 비율 | 1.8% | 0.9% | -0.9%p |

여기서 얻은 결론은 명확했습니다.

- Redis는 "성능 부스터"라기보다 "읽기 압력 분산 장치"
- 키/TTL 규칙이 없으면 오히려 운영 리스크가 커짐

---

## 2) Redis만으로는 해결되지 않았던 문제

읽기 성능은 좋아졌지만, 처리량이 올라가자 다른 병목이 보였습니다.

- API는 빨라졌는데 실제 처리 구간에서 backlog 발생
- 실패 재처리가 흩어져 운영자가 직접 만져야 하는 상황 증가
- 장애 탐지~완화 시간이 길어짐

결국 "조회 최적화"와 "처리 안정화"는 다른 문제였습니다.
이때 Kafka를 도입했습니다.

---

## 3) 2차 개선: Kafka로 API와 처리 경로 분리

구조를 다음처럼 나눴습니다.

- API: 검증 + 저장 + enqueue
- Worker: consume + 발송 + retry/backoff + DLQ

이후 체감이 크게 달라진 부분은 단순 속도가 아니었습니다.
**문제 발생 시 통제력**이 생겼다는 점이 컸습니다.

### Kafka 적용 전/후 (가정 시나리오 데이터)

| 지표 | Redis 단계(적용 전) | Kafka 분리 후 | 변화 |
|---|---:|---:|---:|
| API p95 응답시간 | 460ms | 220ms | -52.2% |
| 피크 consumer lag | 9,800 | 2,300 | -76.5% |
| 재처리 성공률(24h) | 68% | 96% | +28.0%p |
| 장애 감지~완화 평균 시간 | 18분 | 7분 | -61.1% |

핵심은 이거였습니다.

- API는 요청 접수에 집중
- 실패 건은 DLQ로 격리
- 운영자는 대시보드 지표로 병목 지점을 빠르게 확인

즉, "빨라진 시스템"이 아니라 "복구 가능한 시스템"에 가까워졌습니다.

---

## 4) 현재 코드 기준 실측 스냅샷

아래는 현재 로컬 환경에서 실제 확인한 값입니다.

- 실행 명령

```bash
curl -sS "http://localhost:8080/api/v1/notifications/metrics"
```

- 응답

```json
{"success":true,"data":{"pendingTasks":0,"sendingTasks":0,"sentTasks":11,"failedTasks":0,"dlqTasks":1,"requestQueuedLag":0,"malformedQueuedLag":-1,"successRate":91.67,"averageLatencyMs":0.05},"message":null,"code":null}
```

제가 이 수치에서 보는 포인트는:

- `requestQueuedLag=0`: 현재 메인 큐 적체 없음
- `failedTasks=0`, `dlqTasks=1`: 실패를 유실하지 않고 관리하는 흐름 유지
- `successRate=91.67%`: 데모 수준으론 안정적, 상용이라면 추가 개선 여지 있음

---

## 5) 이 과정을 통해 얻은 가장 큰 교훈

예전에는 성능 개선을 "쿼리 몇 ms 줄이기"로 생각했습니다.
지금은 이렇게 정의합니다.

> 성능 개선 = 구조 분리 + 운영 규칙 + 재측정 습관

그리고 개인적으로 가장 컸던 변화는,
문제가 생겼을 때 "어디를 봐야 하는지"가 명확해졌다는 점입니다.

---

## 6) 다음 단계

아직 부족한 것도 많습니다.

- 동일 조건 반복 측정(3회+)으로 편차 관리
- 구간별 p95/p99 추세 누적
- 경보 임계치 운영 자동화

다음 글에서는 같은 구조에서
"자동 새로고침 대시보드 + 임계치 알림"까지 붙여서
운영 품질을 한 단계 더 올린 기록을 남겨보려 합니다.

성능 개선은 끝나는 일이 아니라,
조금씩 덜 흔들리는 시스템을 만드는 과정이라는 걸 이번에 제대로 배웠습니다.
