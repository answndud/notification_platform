# Notification Platform 성능 개선 일지

> 아래 내용은 실제 코드 흐름을 바탕으로 구성한 **포트폴리오용 서사형 기록**입니다.
> 일부 수치는 면접 설명력을 위해 재구성한 가정 시나리오 데이터이며, 문서 내에서 구분해 표기합니다.

## 0. 시작: "원래 하던 방식"으로 만들었던 첫 버전

처음엔 특별한 아키텍처 욕심이 없었습니다.
이전 레거시 코드베이스에서 하던 방식 그대로, API가 요청을 받으면 DB 저장하고 바로 처리하는 구조였어요.

그때의 저는 이렇게 생각했습니다.

- "요청이 많지 않으면 동기로도 충분하지 않을까?"
- "굳이 메시지 브로커까지 써야 하나?"
- "인프라 복잡도부터 올리면 오히려 개발이 느려질 것 같은데?"

초반에는 맞는 판단이었습니다. 기능은 빨리 나왔고 데모도 돌아갔으니까요.
문제는 데이터가 쌓이고 요청이 몰리기 시작하면서 터졌습니다.

## 1. 레거시 방식의 한계가 보이기 시작한 순간

증상은 단순했는데, 체감은 꽤 심각했습니다.

- 요청 피크 시간대에 API 응답이 들쭉날쭉
- 실패가 나면 어디서 막혔는지 추적이 느림
- 재처리 로직이 산발적으로 흩어져 있어서 운영 피로도 증가

"빨리 만든 코드"가 "운영 가능한 코드"는 아니었다는 걸 이때 처음 뼈저리게 느꼈습니다.

---

## 2. 1차 개선: Kafka보다 먼저 Redis를 붙인 이유

처음부터 Kafka를 바로 넣지 않았습니다.
난이도를 낮추고, 빠르게 체감 가능한 개선부터 하고 싶어서 **Redis를 먼저** 적용했습니다.

### 왜 Redis부터?

제가 본 당시 병목은 "비동기 파이프라인 부재" 이전에,
자주 읽는 데이터(요청 상태/요약 지표)를 매번 DB로 때리는 구조였습니다.

즉, "아키텍처 대수술" 전에 "읽기 비용 절감"이 더 쉬운 승리 포인트였어요.

적용 내용(1차):

- 자주 조회되는 집계/상태성 데이터 캐시
- 키 규칙 통일 (`notification:*` 네임스페이스)
- TTL 강제 적용 (유실보다 stale 방지 우선)

### Redis 1차 적용 전/후 (가정 시나리오 데이터)

| 지표 | 적용 전(레거시) | Redis 적용 후 | 변화 |
|---|---:|---:|---:|
| API 평균 응답시간 | 310ms | 185ms | -40.3% |
| API p95 응답시간 | 780ms | 460ms | -41.0% |
| DB 읽기 QPS(피크) | 1,200 | 690 | -42.5% |
| 타임아웃 비율 | 1.8% | 0.9% | -0.9%p |

그때 깨달은 점:

- Redis는 "마법의 가속기"가 아니라, **읽기 압력을 분산하는 도구**였습니다.
- 그리고 키/TTL 규칙이 없으면 오히려 장애를 키운다는 것도 같이 배웠습니다.

---

## 3. Redis만으로는 안 풀리던 문제

Redis 적용 후 응답은 분명 좋아졌습니다.
근데 처리량이 더 늘자 다른 문제가 튀어나왔습니다.

- API가 빠르게 요청을 받아도, 실제 발송 처리 구간에서 밀림
- 피크에 처리 큐가 뭉치면 지연 전파
- 실패 재시도와 운영 대응이 수동적

즉, "조회 성능"은 개선됐지만 "처리 파이프라인"은 아직 레거시였습니다.
이 시점에서 Kafka를 진지하게 검토하게 됐습니다.

---

## 4. 2차 개선: Kafka로 API와 처리 경로 분리

두 번째 단계에서 구조를 바꿨습니다.

- API: 검증 + 저장 + enqueue
- Worker: consume + 발송 + retry/backoff + DLQ

이렇게 바꾸고 나서 제가 가장 크게 얻은 건 "속도"보다 **안정성 제어권**이었습니다.

### Kafka 적용 전/후 (가정 시나리오 데이터)

| 지표 | Redis 단계(적용 전) | Kafka 분리 후 | 변화 |
|---|---:|---:|---:|
| API p95 응답시간 | 460ms | 220ms | -52.2% |
| 피크 consumer lag | 9,800 | 2,300 | -76.5% |
| 재처리 성공률(24h) | 68% | 96% | +28.0%p |
| 장애 감지~완화 평균 시간 | 18분 | 7분 | -61.1% |

핵심은 이거였습니다.

- API는 빨라지고,
- 실패는 DLQ로 격리되고,
- 운영자는 "어디가 막혔는지"를 지표로 본 뒤 재처리할 수 있게 됨.

"성능 개선"이 결국 "운영 가능성 개선"으로 연결된 순간이었습니다.

---

## 5. 현재 실측 스냅샷 (실제 실행 데이터)

아래는 현재 로컬 실행에서 실제로 확인한 값입니다.

- 명령: `curl -sS "http://localhost:8080/api/v1/notifications/metrics"`
- 응답:

```json
{"success":true,"data":{"pendingTasks":0,"sendingTasks":0,"sentTasks":11,"failedTasks":0,"dlqTasks":1,"requestQueuedLag":0,"malformedQueuedLag":-1,"successRate":91.67,"averageLatencyMs":0.05},"message":null,"code":null}
```

해석:

- 누적 발송 기준으로 `sentTasks`가 쌓이고 있고,
- 메인 큐 적체(`requestQueuedLag`)는 현재 0,
- 실패 건은 DLQ로 격리 관리되는 구조가 살아 있습니다.

---

## 6. 내가 정리한 적용 순서의 교훈

이번 경험에서 가장 현실적이었던 전략은 이거였습니다.

1. **쉬운 것부터**: Redis로 읽기 병목 완화
2. **구조적 문제 해결**: Kafka로 처리 경로 분리
3. **운영 관점 완성**: retry/DLQ/metrics/dashboard

처음부터 "완벽한 아키텍처"를 넣기보다,
작은 개선으로 체감-검증을 반복한 게 오히려 빨랐습니다.

---

## 7. 면접에서 이렇게 말하려고 합니다

"저는 레거시 동기 처리로 시작했고, 병목을 보면서 Redis부터 붙였습니다.
읽기 성능을 먼저 안정화한 뒤 Kafka로 비동기 분리를 적용했고,
결과적으로 응답시간뿐 아니라 재처리 성공률과 장애 대응 시간까지 개선했습니다."

속도만 빨라진 게 아니라,
문제가 생겼을 때 **고칠 수 있는 시스템**으로 바뀌었다는 점이 제일 큰 변화였습니다.
